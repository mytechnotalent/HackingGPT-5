{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ae0103",
   "metadata": {},
   "source": [
    "# HackingGPT\n",
    "## Part 5\n",
    "Part 5 covers softmax-based token averaging with masking, the negative infinity trick for masking future positions, understanding how e^(-inf) = 0 enables causal attention, and comparing softmax weights to manual normalization.\n",
    "\n",
    "#### Author: [Kevin Thomas](mailto:ket189@pitt.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0debb921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0ad07",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect the Data\n",
    "Now let's read the file and see what we're working with. Understanding your data is crucial before building any model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "622e793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e89eb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A dim glow rises behind the glass of a screen and the machine exhales in binary tides. The hum is a language and one who listens leans close to catch the quiet grammar. Patterns fold like small maps and seams hint at how the thing holds itself together. Treat each blinking diode and each idle tick as a sentence in a story that asks to be read.\\n\\nThere is patience here, not of haste but of careful unthreading. Where others see a sealed box the curious hand traces the join and wonders which thought made it fit. Do not rush to break, coax the meaning out with questions, and watch how the logic replies in traces and errors and in the echoes of forgotten interfaces.\\n\\nTechnology is artifact and argument at once. It makes a claim about what should be simple, what should be hidden, and what should be trusted. Reverse the gaze and learn its rhetoric, see where it promises ease, where it buries complexity, and where it leaves a backdoor as a sigh between bricks. To read that rhetoric is to be a kind interpreter, not a vandal.\\n\\nThis work is an apprenticeship in humility. Expect bafflement and expect to be corrected by small things, a timing oddity, a mismatch of expectation, a choice that favors speed over grace. Each misstep teaches a vocabulary of trade offs. Each discovery is a map of decisions and not a verdict on worth.\\n\\nThere is a moral keeping in the craft. Let curiosity be tempered with regard for consequence. Let repair and understanding lead rather than exploitation. The skill that opens a lock should also know when to hold the key and when to hand it back, mindful of harm and mindful of help.\\n\\nCelebrate the quiet victories, a stubborn protocol understood, an obscure format rendered speakable, a closed device coaxed into cooperation. These are small reconciliations between human intent and metal will, acts of translation rather than acts of conquest.\\n\\nAfter decoding a mechanism pause and ask what should change, a bug to be fixed, a user to be warned, a design to be amended. The true maker of machines leaves things better for having looked, not simply for having cracked the shell.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527076f2",
   "metadata": {},
   "source": [
    "## Step 2: Version 3 - Using Softmax and Masking\n",
    "In real attention, we use **softmax** to create the weights. This requires a trick where we use `-inf` to mask out future positions (because e^(-inf) = 0).\n",
    "\n",
    "### Why Use Softmax Instead of Manual Division?\n",
    "In Part 4, we normalized by dividing each row by its sum. This works for uniform averaging, but in real transformers the following happens.\n",
    "1. Weights are LEARNED, not uniform.\n",
    "2. Different tokens get different attention weights.\n",
    "3. Softmax naturally converts any values to probabilities that sum to 1.\n",
    "\n",
    "### The Softmax Function\n",
    "Softmax takes a vector of any real numbers and converts them to probabilities.\n",
    "```\n",
    "softmax([x1, x2, x3]) = [e^x1, e^x2, e^x3] / (e^x1 + e^x2 + e^x3)\n",
    "```\n",
    "\n",
    "Properties of softmax are as follows.\n",
    "1. All outputs are positive (because e^x > 0 for any x).\n",
    "2. All outputs sum to 1 (because we divide by total).\n",
    "3. Larger inputs get larger probabilities.\n",
    "4. e^(-inf) = 0, so -inf inputs become 0 probability.\n",
    "\n",
    "### The Masking Trick\n",
    "To prevent looking at future tokens, we set future positions to -inf BEFORE applying softmax.\n",
    "| Position | Raw weights | After masking | After softmax |\n",
    "|----------|-------------|---------------|---------------|\n",
    "| row 0 | [0, 0, 0, 0, 0, 0, 0, 0] | [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf] | [1.0, 0, 0, 0, 0, 0, 0, 0] |\n",
    "| row 1 | [0, 0, 0, 0, 0, 0, 0, 0] | [0, 0, -inf, -inf, -inf, -inf, -inf, -inf] | [0.5, 0.5, 0, 0, 0, 0, 0, 0] |\n",
    "| row 2 | [0, 0, 0, 0, 0, 0, 0, 0] | [0, 0, 0, -inf, -inf, -inf, -inf, -inf] | [0.33, 0.33, 0.33, 0, 0, 0, 0, 0] |\n",
    "| row 3 | [0, 0, 0, 0, 0, 0, 0, 0] | [0, 0, 0, 0, -inf, -inf, -inf, -inf] | [0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0] |\n",
    "| row 4 | [0, 0, 0, 0, 0, 0, 0, 0] | [0, 0, 0, 0, 0, -inf, -inf, -inf] | [0.2, 0.2, 0.2, 0.2, 0.2, 0, 0, 0] |\n",
    "| row 5 | [0, 0, 0, 0, 0, 0, 0, 0] | [0, 0, 0, 0, 0, 0, -inf, -inf] | [0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0, 0] |\n",
    "| row 6 | [0, 0, 0, 0, 0, 0, 0, 0] | [0, 0, 0, 0, 0, 0, 0, -inf] | [0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0] |\n",
    "| row 7 | [0, 0, 0, 0, 0, 0, 0, 0] | [0, 0, 0, 0, 0, 0, 0, 0] | [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68971f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ab4e4f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40292059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define batch dimension\n",
    "B = 4  # batch size: 4 independent sequences\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "319512a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define time dimension\n",
    "T = 8  # sequence length: 8 tokens/positions in each sequence\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "028b539d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define channel dimension\n",
    "C = 2  # feature size: 2 features per token\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dca1cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9269,  1.4873],\n",
       "         [ 0.9007, -2.1055],\n",
       "         [ 0.6784, -1.2345],\n",
       "         [-0.0431, -1.6047],\n",
       "         [-0.7521,  1.6487],\n",
       "         [-0.3925, -1.4036],\n",
       "         [-0.7279, -0.5594],\n",
       "         [-0.7688,  0.7624]],\n",
       "\n",
       "        [[ 1.6423, -0.1596],\n",
       "         [-0.4974,  0.4396],\n",
       "         [-0.7581,  1.0783],\n",
       "         [ 0.8008,  1.6806],\n",
       "         [ 1.2791,  1.2964],\n",
       "         [ 0.6105,  1.3347],\n",
       "         [-0.2316,  0.0418],\n",
       "         [-0.2516,  0.8599]],\n",
       "\n",
       "        [[-1.3847, -0.8712],\n",
       "         [-0.2234,  1.7174],\n",
       "         [ 0.3189, -0.4245],\n",
       "         [ 0.3057, -0.7746],\n",
       "         [-1.5576,  0.9956],\n",
       "         [-0.8798, -0.6011],\n",
       "         [-1.2742,  2.1228],\n",
       "         [-1.2347, -0.4879]],\n",
       "\n",
       "        [[-0.9138, -0.6581],\n",
       "         [ 0.0780,  0.5258],\n",
       "         [-0.4880,  1.1914],\n",
       "         [-0.8140, -0.7360],\n",
       "         [-1.4032,  0.0360],\n",
       "         [-0.0635,  0.6756],\n",
       "         [-0.0978,  1.8446],\n",
       "         [-1.1845,  1.3835]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with random data\n",
    "x = torch.randn(B, T, C)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f830c184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the mask (lower triangular)\n",
    "# this is the same lower triangular matrix from Part 4\n",
    "# 1s where we CAN look, 0s where we CANNOT look\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d47ee25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examining each row of the lower triangular mask\n",
      "\n",
      "row 0: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "       position 0 can only see itself\n",
      "       1 = can look, 0 = cannot look\n",
      "\n",
      "row 1: [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "       position 1 can see positions 0 and 1\n",
      "\n",
      "row 2: [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "       position 2 can see positions 0, 1, and 2\n",
      "\n",
      "row 3: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "       position 3 can see positions 0, 1, 2, and 3\n",
      "\n",
      "row 4: [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "       position 4 can see positions 0, 1, 2, 3, and 4\n",
      "\n",
      "row 5: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]\n",
      "       position 5 can see positions 0, 1, 2, 3, 4, and 5\n",
      "\n",
      "row 6: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n",
      "       position 6 can see positions 0, 1, 2, 3, 4, 5, and 6\n",
      "\n",
      "row 7: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "       position 7 can see all positions 0, 1, 2, 3, 4, 5, 6, and 7\n"
     ]
    }
   ],
   "source": [
    "# examine each row of tril to understand the mask\n",
    "print('examining each row of the lower triangular mask')\n",
    "print()\n",
    "print(f'row 0: {tril[0].tolist()}')\n",
    "print('       position 0 can only see itself')\n",
    "print('       1 = can look, 0 = cannot look')\n",
    "print()\n",
    "print(f'row 1: {tril[1].tolist()}')\n",
    "print('       position 1 can see positions 0 and 1')\n",
    "print()\n",
    "print(f'row 2: {tril[2].tolist()}')\n",
    "print('       position 2 can see positions 0, 1, and 2')\n",
    "print()\n",
    "print(f'row 3: {tril[3].tolist()}')\n",
    "print('       position 3 can see positions 0, 1, 2, and 3')\n",
    "print()\n",
    "print(f'row 4: {tril[4].tolist()}')\n",
    "print('       position 4 can see positions 0, 1, 2, 3, and 4')\n",
    "print()\n",
    "print(f'row 5: {tril[5].tolist()}')\n",
    "print('       position 5 can see positions 0, 1, 2, 3, 4, and 5')\n",
    "print()\n",
    "print(f'row 6: {tril[6].tolist()}')\n",
    "print('       position 6 can see positions 0, 1, 2, 3, 4, 5, and 6')\n",
    "print()\n",
    "print(f'row 7: {tril[7].tolist()}')\n",
    "print('       position 7 can see all positions 0, 1, 2, 3, 4, 5, 6, and 7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e018cc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with zeros (equal weights before softmax)\n",
    "# in real attention, these would be LEARNED values\n",
    "# for now, we use zeros to show the mechanism\n",
    "wei = torch.zeros((T, T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c359c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding the initial weights\n",
      "\n",
      "wei is an 8x8 matrix of zeros\n",
      "this represents: every position has EQUAL affinity for every other position\n",
      "\n",
      "why zeros?\n",
      "   when we apply softmax to equal values, we get equal probabilities\n",
      "   softmax([0, 0, 0]) = [0.333, 0.333, 0.333]\n",
      "   softmax([0, 0]) = [0.5, 0.5]\n",
      "   softmax([0]) = [1.0]\n",
      "\n",
      "in real transformers, these values would be LEARNED\n",
      "different values would give different attention patterns\n"
     ]
    }
   ],
   "source": [
    "# understand why we start with zeros\n",
    "print('understanding the initial weights')\n",
    "print()\n",
    "print('wei is an 8x8 matrix of zeros')\n",
    "print('this represents: every position has EQUAL affinity for every other position')\n",
    "print()\n",
    "print('why zeros?')\n",
    "print('   when we apply softmax to equal values, we get equal probabilities')\n",
    "print('   softmax([0, 0, 0]) = [0.333, 0.333, 0.333]')\n",
    "print('   softmax([0, 0]) = [0.5, 0.5]')\n",
    "print('   softmax([0]) = [1.0]')\n",
    "print()\n",
    "print('in real transformers, these values would be LEARNED')\n",
    "print('different values would give different attention patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73392704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set future positions to -infinity\n",
    "# masked_fill: where tril==0, fill with -inf\n",
    "# this prevents looking at future tokens\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71ec326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding masked_fill\n",
      "\n",
      "tril == 0 creates a boolean mask\n",
      "True where tril is 0 (future positions)\n",
      "False where tril is 1 (past/current positions)\n",
      "\n",
      "tril == 0:\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False]])\n",
      "\n",
      "masked_fill replaces values where the mask is True with -inf\n"
     ]
    }
   ],
   "source": [
    "# understand the masked_fill operation step by step\n",
    "print('understanding masked_fill')\n",
    "print()\n",
    "print('tril == 0 creates a boolean mask')\n",
    "print('True where tril is 0 (future positions)')\n",
    "print('False where tril is 1 (past/current positions)')\n",
    "print()\n",
    "mask = tril == 0\n",
    "print('tril == 0:')\n",
    "print(mask)\n",
    "print()\n",
    "print('masked_fill replaces values where the mask is True with -inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bb41386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examining each row after masking with -inf\n",
      "\n",
      "row 0: [0.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "       only position 0 is visible (0), positions 1-7 are masked (-inf)\n",
      "\n",
      "row 1: [0.0, 0.0, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "       positions 0-1 are visible (0), positions 2-7 are masked (-inf)\n",
      "\n",
      "row 2: [0.0, 0.0, 0.0, -inf, -inf, -inf, -inf, -inf]\n",
      "       positions 0-2 are visible (0), positions 3-7 are masked (-inf)\n",
      "\n",
      "row 3: [0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf, -inf]\n",
      "       positions 0-3 are visible (0), positions 4-7 are masked (-inf)\n",
      "\n",
      "row 4: [0.0, 0.0, 0.0, 0.0, 0.0, -inf, -inf, -inf]\n",
      "       positions 0-4 are visible (0), positions 5-7 are masked (-inf)\n",
      "\n",
      "row 5: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -inf, -inf]\n",
      "       positions 0-5 are visible (0), positions 6-7 are masked (-inf)\n",
      "\n",
      "row 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -inf]\n",
      "       positions 0-6 are visible (0), position 7 is masked (-inf)\n",
      "\n",
      "row 7: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "       all positions 0-7 are visible (0), nothing is masked\n"
     ]
    }
   ],
   "source": [
    "# examine each row of wei after masking\n",
    "print('examining each row after masking with -inf')\n",
    "print()\n",
    "print(f'row 0: {wei[0].tolist()}')\n",
    "print('       only position 0 is visible (0), positions 1-7 are masked (-inf)')\n",
    "print()\n",
    "print(f'row 1: {wei[1].tolist()}')\n",
    "print('       positions 0-1 are visible (0), positions 2-7 are masked (-inf)')\n",
    "print()\n",
    "print(f'row 2: {wei[2].tolist()}')\n",
    "print('       positions 0-2 are visible (0), positions 3-7 are masked (-inf)')\n",
    "print()\n",
    "print(f'row 3: {wei[3].tolist()}')\n",
    "print('       positions 0-3 are visible (0), positions 4-7 are masked (-inf)')\n",
    "print()\n",
    "print(f'row 4: {wei[4].tolist()}')\n",
    "print('       positions 0-4 are visible (0), positions 5-7 are masked (-inf)')\n",
    "print()\n",
    "print(f'row 5: {wei[5].tolist()}')\n",
    "print('       positions 0-5 are visible (0), positions 6-7 are masked (-inf)')\n",
    "print()\n",
    "print(f'row 6: {wei[6].tolist()}')\n",
    "print('       positions 0-6 are visible (0), position 7 is masked (-inf)')\n",
    "print()\n",
    "print(f'row 7: {wei[7].tolist()}')\n",
    "print('       all positions 0-7 are visible (0), nothing is masked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6e8832e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply softmax\n",
    "# softmax converts values to probabilities that sum to 1\n",
    "# e^(-inf) = 0, so masked positions become 0 probability\n",
    "# dim=-1 means apply softmax along the last dimension (each row)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a171733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding why the -inf trick works\n",
      "\n",
      "softmax formula: softmax(x_i) = e^(x_i) / sum(e^(x_j) for all j)\n",
      "\n",
      "key insight: e^(-inf) = 0\n",
      "\n",
      "examples\n",
      "   e^(0) = 1.0000\n",
      "   e^(1) = 2.7183\n",
      "   e^(-1) = 0.3679\n",
      "   e^(-10) = 0.0000453999\n",
      "   e^(-100) = 0.00000000000000000000000000000000000000000003720076\n",
      "   e^(-inf) = 0 (exactly)\n",
      "\n",
      "so when we set future positions to -inf\n",
      "they become e^(-inf) = 0 in the softmax numerator\n",
      "this means they contribute 0 probability!\n"
     ]
    }
   ],
   "source": [
    "# understand why e^(-inf) = 0\n",
    "print('understanding why the -inf trick works')\n",
    "print()\n",
    "print('softmax formula: softmax(x_i) = e^(x_i) / sum(e^(x_j) for all j)')\n",
    "print()\n",
    "print('key insight: e^(-inf) = 0')\n",
    "print()\n",
    "import math\n",
    "print('examples')\n",
    "print(f'   e^(0) = {math.exp(0):.4f}')\n",
    "print(f'   e^(1) = {math.exp(1):.4f}')\n",
    "print(f'   e^(-1) = {math.exp(-1):.4f}')\n",
    "print(f'   e^(-10) = {math.exp(-10):.10f}')\n",
    "print(f'   e^(-100) = {math.exp(-100):.50f}')\n",
    "print('   e^(-inf) = 0 (exactly)')\n",
    "print()\n",
    "print('so when we set future positions to -inf')\n",
    "print('they become e^(-inf) = 0 in the softmax numerator')\n",
    "print('this means they contribute 0 probability!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69b8b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracing softmax for each row\n",
      "\n",
      "row 0: input = [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "   e^0 = 1, e^(-inf) = 0, e^(-inf) = 0, ...\n",
      "   numerators = [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "   sum = 1\n",
      "   softmax = [1/1, 0/1, 0/1, ...] = [1.0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   actual: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "row 1: input = [0, 0, -inf, -inf, -inf, -inf, -inf, -inf]\n",
      "   e^0 = 1, e^0 = 1, e^(-inf) = 0, ...\n",
      "   numerators = [1, 1, 0, 0, 0, 0, 0, 0]\n",
      "   sum = 2\n",
      "   softmax = [1/2, 1/2, 0/2, ...] = [0.5, 0.5, 0, 0, 0, 0, 0, 0]\n",
      "   actual: [0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "row 2: input = [0, 0, 0, -inf, -inf, -inf, -inf, -inf]\n",
      "   e^0 = 1, e^0 = 1, e^0 = 1, e^(-inf) = 0, ...\n",
      "   numerators = [1, 1, 1, 0, 0, 0, 0, 0]\n",
      "   sum = 3\n",
      "   softmax = [1/3, 1/3, 1/3, 0, ...] â‰ˆ [0.333, 0.333, 0.333, 0, 0, 0, 0, 0]\n",
      "   actual: [0.3333333432674408, 0.3333333432674408, 0.3333333432674408, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# trace through softmax for each row manually\n",
    "print('tracing softmax for each row')\n",
    "print()\n",
    "print('row 0: input = [0, -inf, -inf, -inf, -inf, -inf, -inf, -inf]')\n",
    "print('   e^0 = 1, e^(-inf) = 0, e^(-inf) = 0, ...')\n",
    "print('   numerators = [1, 0, 0, 0, 0, 0, 0, 0]')\n",
    "print('   sum = 1')\n",
    "print('   softmax = [1/1, 0/1, 0/1, ...] = [1.0, 0, 0, 0, 0, 0, 0, 0]')\n",
    "print(f'   actual: {wei[0].tolist()}')\n",
    "print()\n",
    "print('row 1: input = [0, 0, -inf, -inf, -inf, -inf, -inf, -inf]')\n",
    "print('   e^0 = 1, e^0 = 1, e^(-inf) = 0, ...')\n",
    "print('   numerators = [1, 1, 0, 0, 0, 0, 0, 0]')\n",
    "print('   sum = 2')\n",
    "print('   softmax = [1/2, 1/2, 0/2, ...] = [0.5, 0.5, 0, 0, 0, 0, 0, 0]')\n",
    "print(f'   actual: {wei[1].tolist()}')\n",
    "print()\n",
    "print('row 2: input = [0, 0, 0, -inf, -inf, -inf, -inf, -inf]')\n",
    "print('   e^0 = 1, e^0 = 1, e^0 = 1, e^(-inf) = 0, ...')\n",
    "print('   numerators = [1, 1, 1, 0, 0, 0, 0, 0]')\n",
    "print('   sum = 3')\n",
    "print('   softmax = [1/3, 1/3, 1/3, 0, ...] â‰ˆ [0.333, 0.333, 0.333, 0, 0, 0, 0, 0]')\n",
    "print(f'   actual: {wei[2].tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "969c2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuing softmax trace\n",
      "\n",
      "row 3: input = [0, 0, 0, 0, -inf, -inf, -inf, -inf]\n",
      "   numerators = [1, 1, 1, 1, 0, 0, 0, 0]\n",
      "   sum = 4\n",
      "   softmax = [1/4, 1/4, 1/4, 1/4, 0, 0, 0, 0] = [0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0]\n",
      "   actual: [0.25, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "row 4: input = [0, 0, 0, 0, 0, -inf, -inf, -inf]\n",
      "   numerators = [1, 1, 1, 1, 1, 0, 0, 0]\n",
      "   sum = 5\n",
      "   softmax = [1/5, 1/5, 1/5, 1/5, 1/5, 0, 0, 0] = [0.2, 0.2, 0.2, 0.2, 0.2, 0, 0, 0]\n",
      "   actual: [0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.0, 0.0, 0.0]\n",
      "\n",
      "row 5: input = [0, 0, 0, 0, 0, 0, -inf, -inf]\n",
      "   numerators = [1, 1, 1, 1, 1, 1, 0, 0]\n",
      "   sum = 6\n",
      "   softmax = [1/6, ...] â‰ˆ [0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0, 0]\n",
      "   actual: [0.1666666716337204, 0.1666666716337204, 0.1666666716337204, 0.1666666716337204, 0.1666666716337204, 0.1666666716337204, 0.0, 0.0]\n",
      "\n",
      "row 6: input = [0, 0, 0, 0, 0, 0, 0, -inf]\n",
      "   numerators = [1, 1, 1, 1, 1, 1, 1, 0]\n",
      "   sum = 7\n",
      "   softmax = [1/7, ...] â‰ˆ [0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0]\n",
      "   actual: [0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.0]\n",
      "\n",
      "row 7: input = [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   numerators = [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "   sum = 8\n",
      "   softmax = [1/8, ...] = [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]\n",
      "   actual: [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]\n"
     ]
    }
   ],
   "source": [
    "# continue tracing softmax for remaining rows\n",
    "print('continuing softmax trace')\n",
    "print()\n",
    "print('row 3: input = [0, 0, 0, 0, -inf, -inf, -inf, -inf]')\n",
    "print('   numerators = [1, 1, 1, 1, 0, 0, 0, 0]')\n",
    "print('   sum = 4')\n",
    "print('   softmax = [1/4, 1/4, 1/4, 1/4, 0, 0, 0, 0] = [0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0]')\n",
    "print(f'   actual: {wei[3].tolist()}')\n",
    "print()\n",
    "print('row 4: input = [0, 0, 0, 0, 0, -inf, -inf, -inf]')\n",
    "print('   numerators = [1, 1, 1, 1, 1, 0, 0, 0]')\n",
    "print('   sum = 5')\n",
    "print('   softmax = [1/5, 1/5, 1/5, 1/5, 1/5, 0, 0, 0] = [0.2, 0.2, 0.2, 0.2, 0.2, 0, 0, 0]')\n",
    "print(f'   actual: {wei[4].tolist()}')\n",
    "print()\n",
    "print('row 5: input = [0, 0, 0, 0, 0, 0, -inf, -inf]')\n",
    "print('   numerators = [1, 1, 1, 1, 1, 1, 0, 0]')\n",
    "print('   sum = 6')\n",
    "print('   softmax = [1/6, ...] â‰ˆ [0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0, 0]')\n",
    "print(f'   actual: {wei[5].tolist()}')\n",
    "print()\n",
    "print('row 6: input = [0, 0, 0, 0, 0, 0, 0, -inf]')\n",
    "print('   numerators = [1, 1, 1, 1, 1, 1, 1, 0]')\n",
    "print('   sum = 7')\n",
    "print('   softmax = [1/7, ...] â‰ˆ [0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0]')\n",
    "print(f'   actual: {wei[6].tolist()}')\n",
    "print()\n",
    "print('row 7: input = [0, 0, 0, 0, 0, 0, 0, 0]')\n",
    "print('   numerators = [1, 1, 1, 1, 1, 1, 1, 1]')\n",
    "print('   sum = 8')\n",
    "print('   softmax = [1/8, ...] = [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]')\n",
    "print(f'   actual: {wei[7].tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7e43ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify: each row sums to 1.0\n",
      "\n",
      "row 0 sum: 1.0000\n",
      "row 1 sum: 1.0000\n",
      "row 2 sum: 1.0000\n",
      "row 3 sum: 1.0000\n",
      "row 4 sum: 1.0000\n",
      "row 5 sum: 1.0000\n",
      "row 6 sum: 1.0000\n",
      "row 7 sum: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# verify each row sums to 1\n",
    "print('verify: each row sums to 1.0')\n",
    "print()\n",
    "for i in range(T):\n",
    "    row_sum = wei[i].sum().item()\n",
    "    print(f'row {i} sum: {row_sum:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75c4692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Why softmax with masking?\n",
      "   - In real attention, \"wei\" will not be all zeros\n",
      "   - It will have LEARNED, data-dependent values\n",
      "   - But we still need to mask future â†’ -inf trick works!\n",
      "\n",
      "example: imagine learned attention weights\n",
      "   raw weights = [0.5, 0.8, 0.2, 0.9, ...]\n",
      "   after masking row 1 = [0.5, 0.8, -inf, -inf, ...]\n",
      "   softmax only considers 0.5 and 0.8\n",
      "   result = [e^0.5 / (e^0.5 + e^0.8), e^0.8 / (e^0.5 + e^0.8), 0, 0, ...]\n",
      "\n",
      "the -inf masking trick works regardless of what the original values are\n"
     ]
    }
   ],
   "source": [
    "# why softmax with masking is used in real attention\n",
    "print('ðŸ’¡ Why softmax with masking?')\n",
    "print('   - In real attention, \"wei\" will not be all zeros')\n",
    "print('   - It will have LEARNED, data-dependent values')\n",
    "print('   - But we still need to mask future â†’ -inf trick works!')\n",
    "print()\n",
    "print('example: imagine learned attention weights')\n",
    "print('   raw weights = [0.5, 0.8, 0.2, 0.9, ...]')\n",
    "print('   after masking row 1 = [0.5, 0.8, -inf, -inf, ...]')\n",
    "print('   softmax only considers 0.5 and 0.8')\n",
    "print('   result = [e^0.5 / (e^0.5 + e^0.8), e^0.8 / (e^0.5 + e^0.8), 0, 0, ...]')\n",
    "print()\n",
    "print('the -inf masking trick works regardless of what the original values are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc6b4ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9269,  1.4873],\n",
       "         [ 1.4138, -0.3091],\n",
       "         [ 1.1687, -0.6176],\n",
       "         [ 0.8657, -0.8644],\n",
       "         [ 0.5422, -0.3617],\n",
       "         [ 0.3864, -0.5354],\n",
       "         [ 0.2272, -0.5388],\n",
       "         [ 0.1027, -0.3762]],\n",
       "\n",
       "        [[ 1.6423, -0.1596],\n",
       "         [ 0.5725,  0.1400],\n",
       "         [ 0.1289,  0.4528],\n",
       "         [ 0.2969,  0.7597],\n",
       "         [ 0.4933,  0.8671],\n",
       "         [ 0.5129,  0.9450],\n",
       "         [ 0.4065,  0.8160],\n",
       "         [ 0.3242,  0.8215]],\n",
       "\n",
       "        [[-1.3847, -0.8712],\n",
       "         [-0.8040,  0.4231],\n",
       "         [-0.4297,  0.1405],\n",
       "         [-0.2459, -0.0882],\n",
       "         [-0.5082,  0.1285],\n",
       "         [-0.5701,  0.0069],\n",
       "         [-0.6707,  0.3092],\n",
       "         [-0.7412,  0.2095]],\n",
       "\n",
       "        [[-0.9138, -0.6581],\n",
       "         [-0.4179, -0.0662],\n",
       "         [-0.4413,  0.3530],\n",
       "         [-0.5344,  0.0808],\n",
       "         [-0.7082,  0.0718],\n",
       "         [-0.6008,  0.1724],\n",
       "         [-0.5289,  0.4113],\n",
       "         [-0.6109,  0.5329]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply to get weighted averages\n",
    "# wei @ x performs the weighted averaging\n",
    "# wei shape: (T, T) = (8, 8)\n",
    "# x shape: (B, T, C) = (4, 8, 2)\n",
    "# result shape: (B, T, C) = (4, 8, 2)\n",
    "x_bow_3 = wei @ x\n",
    "x_bow_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33229d0a",
   "metadata": {},
   "source": [
    "### Understanding the Matrix Multiplication with Softmax Weights\n",
    "When we do `wei @ x`, PyTorch broadcasts the operation.\n",
    "- `wei` has shape (T, T) = (8, 8)\n",
    "- `x` has shape (B, T, C) = (4, 8, 2)\n",
    "\n",
    "PyTorch treats the batch dimension (B=4) specially. It performs 4 separate matrix multiplications.\n",
    "- `wei @ x[0]` â†’ result for batch 0\n",
    "- `wei @ x[1]` â†’ result for batch 1  \n",
    "- `wei @ x[2]` â†’ result for batch 2\n",
    "- `wei @ x[3]` â†’ result for batch 3\n",
    "\n",
    "For each batch, the multiplication is the following.\n",
    "- (8, 8) @ (8, 2) = (8, 2)\n",
    "\n",
    "The final result has shape (4, 8, 2) = (B, T, C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2f5693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding the matrix multiplication for batch 0\n",
      "\n",
      "wei shape: torch.Size([8, 8])\n",
      "x[0] shape: torch.Size([8, 2])\n",
      "\n",
      "x[0] (the input for batch 0)\n",
      "tensor([[ 1.9269,  1.4873],\n",
      "        [ 0.9007, -2.1055],\n",
      "        [ 0.6784, -1.2345],\n",
      "        [-0.0431, -1.6047],\n",
      "        [-0.7521,  1.6487],\n",
      "        [-0.3925, -1.4036],\n",
      "        [-0.7279, -0.5594],\n",
      "        [-0.7688,  0.7624]])\n",
      "\n",
      "wei (the softmax weight matrix)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# let's trace through the matrix multiplication step by step for batch 0\n",
    "print('understanding the matrix multiplication for batch 0')\n",
    "print()\n",
    "print(f'wei shape: {wei.shape}')\n",
    "print(f'x[0] shape: {x[0].shape}')\n",
    "print()\n",
    "print('x[0] (the input for batch 0)')\n",
    "print(x[0])\n",
    "print()\n",
    "print('wei (the softmax weight matrix)')\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4817a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position 0 calculation\n",
      "\n",
      "wei[0] = [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "this means: 1.0 * x[0,0] + 0.0 * x[0,1] + 0.0 * x[0,2] + ... + 0.0 * x[0,7]\n",
      "\n",
      "for feature 0\n",
      "   1.0000 * 1.9269 = 1.9269\n",
      "\n",
      "for feature 1\n",
      "   1.0000 * 1.4873 = 1.4873\n",
      "\n",
      "result: x_bow_3[0, 0] = [1.9269150495529175, 1.4872841835021973]\n",
      "verify: x[0, 0]       = [1.9269150495529175, 1.4872841835021973]\n",
      "(position 0 just equals itself since it only sees itself)\n"
     ]
    }
   ],
   "source": [
    "# position 0 calculation (row 0 of wei @ x[0])\n",
    "print('position 0 calculation')\n",
    "print()\n",
    "print(f'wei[0] = {wei[0].tolist()}')\n",
    "print(f'this means: 1.0 * x[0,0] + 0.0 * x[0,1] + 0.0 * x[0,2] + ... + 0.0 * x[0,7]')\n",
    "print()\n",
    "print('for feature 0')\n",
    "val = wei[0, 0].item() * x[0, 0, 0].item()\n",
    "print(f'   {wei[0, 0].item():.4f} * {x[0, 0, 0].item():.4f} = {val:.4f}')\n",
    "print()\n",
    "print('for feature 1')\n",
    "val = wei[0, 0].item() * x[0, 0, 1].item()\n",
    "print(f'   {wei[0, 0].item():.4f} * {x[0, 0, 1].item():.4f} = {val:.4f}')\n",
    "print()\n",
    "print(f'result: x_bow_3[0, 0] = {x_bow_3[0, 0].tolist()}')\n",
    "print(f'verify: x[0, 0]       = {x[0, 0].tolist()}')\n",
    "print('(position 0 just equals itself since it only sees itself)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d0dc378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position 1 calculation\n",
      "\n",
      "wei[1] = [0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "this means: 0.5 * x[0,0] + 0.5 * x[0,1] + 0.0 * x[0,2] + ... + 0.0 * x[0,7]\n",
      "\n",
      "for feature 0\n",
      "   0.5000 * 1.9269 = 0.9635\n",
      " + 0.5000 * 0.9007 = 0.4504\n",
      "   sum = 1.4138\n",
      "\n",
      "for feature 1\n",
      "   0.5000 * 1.4873 = 0.7436\n",
      " + 0.5000 * -2.1055 = -1.0528\n",
      "   sum = -0.3091\n",
      "\n",
      "result: x_bow_3[0, 1] = [1.4138160943984985, -0.3091186285018921]\n",
      "\n",
      "manual verification\n",
      "(x[0,0] + x[0,1]) / 2 = [1.4138160943984985, -0.3091186285018921]\n"
     ]
    }
   ],
   "source": [
    "# position 1 calculation (row 1 of wei @ x[0])\n",
    "print('position 1 calculation')\n",
    "print()\n",
    "print(f'wei[1] = {wei[1].tolist()}')\n",
    "print(f'this means: 0.5 * x[0,0] + 0.5 * x[0,1] + 0.0 * x[0,2] + ... + 0.0 * x[0,7]')\n",
    "print()\n",
    "print('for feature 0')\n",
    "val0 = wei[1, 0].item() * x[0, 0, 0].item()\n",
    "val1 = wei[1, 1].item() * x[0, 1, 0].item()\n",
    "print(f'   {wei[1, 0].item():.4f} * {x[0, 0, 0].item():.4f} = {val0:.4f}')\n",
    "print(f' + {wei[1, 1].item():.4f} * {x[0, 1, 0].item():.4f} = {val1:.4f}')\n",
    "print(f'   sum = {val0 + val1:.4f}')\n",
    "print()\n",
    "print('for feature 1')\n",
    "val0 = wei[1, 0].item() * x[0, 0, 1].item()\n",
    "val1 = wei[1, 1].item() * x[0, 1, 1].item()\n",
    "print(f'   {wei[1, 0].item():.4f} * {x[0, 0, 1].item():.4f} = {val0:.4f}')\n",
    "print(f' + {wei[1, 1].item():.4f} * {x[0, 1, 1].item():.4f} = {val1:.4f}')\n",
    "print(f'   sum = {val0 + val1:.4f}')\n",
    "print()\n",
    "print(f'result: x_bow_3[0, 1] = {x_bow_3[0, 1].tolist()}')\n",
    "print()\n",
    "print('manual verification')\n",
    "manual_avg = (x[0, 0] + x[0, 1]) / 2\n",
    "print(f'(x[0,0] + x[0,1]) / 2 = {manual_avg.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "516da749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position 2 calculation\n",
      "\n",
      "wei[2] = [0.3333333432674408, 0.3333333432674408, 0.3333333432674408, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "this means: 0.333 * x[0,0] + 0.333 * x[0,1] + 0.333 * x[0,2] + 0.0 * x[0,3] + ...\n",
      "\n",
      "for feature 0\n",
      "   0.3333 * 1.9269 = 0.6423\n",
      " + 0.3333 * 0.9007 = 0.3002\n",
      " + 0.3333 * 0.6784 = 0.2261\n",
      "   sum = 1.1687\n",
      "\n",
      "for feature 1\n",
      "   0.3333 * 1.4873 = 0.4958\n",
      " + 0.3333 * -2.1055 = -0.7018\n",
      " + 0.3333 * -1.2345 = -0.4115\n",
      "   sum = -0.6176\n",
      "\n",
      "result: x_bow_3[0, 2] = [1.168683648109436, -0.6175941228866577]\n",
      "\n",
      "manual verification\n",
      "(x[0,0] + x[0,1] + x[0,2]) / 3 = [1.1686835289001465, -0.6175940632820129]\n"
     ]
    }
   ],
   "source": [
    "# position 2 calculation (row 2 of wei @ x[0])\n",
    "print('position 2 calculation')\n",
    "print()\n",
    "print(f'wei[2] = {wei[2].tolist()}')\n",
    "print(f'this means: 0.333 * x[0,0] + 0.333 * x[0,1] + 0.333 * x[0,2] + 0.0 * x[0,3] + ...')\n",
    "print()\n",
    "print('for feature 0')\n",
    "val0 = wei[2, 0].item() * x[0, 0, 0].item()\n",
    "val1 = wei[2, 1].item() * x[0, 1, 0].item()\n",
    "val2 = wei[2, 2].item() * x[0, 2, 0].item()\n",
    "print(f'   {wei[2, 0].item():.4f} * {x[0, 0, 0].item():.4f} = {val0:.4f}')\n",
    "print(f' + {wei[2, 1].item():.4f} * {x[0, 1, 0].item():.4f} = {val1:.4f}')\n",
    "print(f' + {wei[2, 2].item():.4f} * {x[0, 2, 0].item():.4f} = {val2:.4f}')\n",
    "print(f'   sum = {val0 + val1 + val2:.4f}')\n",
    "print()\n",
    "print('for feature 1')\n",
    "val0 = wei[2, 0].item() * x[0, 0, 1].item()\n",
    "val1 = wei[2, 1].item() * x[0, 1, 1].item()\n",
    "val2 = wei[2, 2].item() * x[0, 2, 1].item()\n",
    "print(f'   {wei[2, 0].item():.4f} * {x[0, 0, 1].item():.4f} = {val0:.4f}')\n",
    "print(f' + {wei[2, 1].item():.4f} * {x[0, 1, 1].item():.4f} = {val1:.4f}')\n",
    "print(f' + {wei[2, 2].item():.4f} * {x[0, 2, 1].item():.4f} = {val2:.4f}')\n",
    "print(f'   sum = {val0 + val1 + val2:.4f}')\n",
    "print()\n",
    "print(f'result: x_bow_3[0, 2] = {x_bow_3[0, 2].tolist()}')\n",
    "print()\n",
    "print('manual verification')\n",
    "manual_avg = (x[0, 0] + x[0, 1] + x[0, 2]) / 3\n",
    "print(f'(x[0,0] + x[0,1] + x[0,2]) / 3 = {manual_avg.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "476816cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position 3 calculation\n",
      "\n",
      "wei[3] = [0.25, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0]\n",
      "this means: 0.25 * x[0,0] + 0.25 * x[0,1] + 0.25 * x[0,2] + 0.25 * x[0,3] + 0.0 * ...\n",
      "\n",
      "for feature 0\n",
      "   0.2500 * 1.9269 = 0.4817\n",
      " + 0.2500 * 0.9007 = 0.2252\n",
      " + 0.2500 * 0.6784 = 0.1696\n",
      " + 0.2500 * -0.0431 = -0.0108\n",
      "   sum = 0.8657\n",
      "\n",
      "for feature 1\n",
      "   0.2500 * 1.4873 = 0.3718\n",
      " + 0.2500 * -2.1055 = -0.5264\n",
      " + 0.2500 * -1.2345 = -0.3086\n",
      " + 0.2500 * -1.6047 = -0.4012\n",
      "   sum = -0.8644\n",
      "\n",
      "result: x_bow_3[0, 3] = [0.8657457828521729, -0.8643622994422913]\n",
      "\n",
      "manual verification\n",
      "(x[0,0] + x[0,1] + x[0,2] + x[0,3]) / 4 = [0.8657457828521729, -0.8643622994422913]\n"
     ]
    }
   ],
   "source": [
    "# position 3 calculation (row 3 of wei @ x[0])\n",
    "print('position 3 calculation')\n",
    "print()\n",
    "print(f'wei[3] = {wei[3].tolist()}')\n",
    "print(f'this means: 0.25 * x[0,0] + 0.25 * x[0,1] + 0.25 * x[0,2] + 0.25 * x[0,3] + 0.0 * ...')\n",
    "print()\n",
    "print('for feature 0')\n",
    "vals = [wei[3, i].item() * x[0, i, 0].item() for i in range(4)]\n",
    "for i in range(4):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[3, i].item():.4f} * {x[0, i, 0].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print('for feature 1')\n",
    "vals = [wei[3, i].item() * x[0, i, 1].item() for i in range(4)]\n",
    "for i in range(4):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[3, i].item():.4f} * {x[0, i, 1].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print(f'result: x_bow_3[0, 3] = {x_bow_3[0, 3].tolist()}')\n",
    "print()\n",
    "print('manual verification')\n",
    "manual_avg = (x[0, 0] + x[0, 1] + x[0, 2] + x[0, 3]) / 4\n",
    "print(f'(x[0,0] + x[0,1] + x[0,2] + x[0,3]) / 4 = {manual_avg.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23db6393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position 4 calculation\n",
      "\n",
      "wei[4] = [0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.0, 0.0, 0.0]\n",
      "this means: 0.2 * x[0,0] + 0.2 * x[0,1] + 0.2 * x[0,2] + 0.2 * x[0,3] + 0.2 * x[0,4] + 0.0 * ...\n",
      "\n",
      "for feature 0\n",
      "   0.2000 * 1.9269 = 0.3854\n",
      " + 0.2000 * 0.9007 = 0.1801\n",
      " + 0.2000 * 0.6784 = 0.1357\n",
      " + 0.2000 * -0.0431 = -0.0086\n",
      " + 0.2000 * -0.7521 = -0.1504\n",
      "   sum = 0.5422\n",
      "\n",
      "for feature 1\n",
      "   0.2000 * 1.4873 = 0.2975\n",
      " + 0.2000 * -2.1055 = -0.4211\n",
      " + 0.2000 * -1.2345 = -0.2469\n",
      " + 0.2000 * -1.6047 = -0.3209\n",
      " + 0.2000 * 1.6487 = 0.3297\n",
      "   sum = -0.3617\n",
      "\n",
      "result: x_bow_3[0, 4] = [0.542169451713562, -0.36174529790878296]\n",
      "\n",
      "manual verification\n",
      "(x[0,0] + x[0,1] + x[0,2] + x[0,3] + x[0,4]) / 5 = [0.5421693921089172, -0.36174526810646057]\n"
     ]
    }
   ],
   "source": [
    "# position 4 calculation (row 4 of wei @ x[0])\n",
    "print('position 4 calculation')\n",
    "print()\n",
    "print(f'wei[4] = {wei[4].tolist()}')\n",
    "print(f'this means: 0.2 * x[0,0] + 0.2 * x[0,1] + 0.2 * x[0,2] + 0.2 * x[0,3] + 0.2 * x[0,4] + 0.0 * ...')\n",
    "print()\n",
    "print('for feature 0')\n",
    "vals = [wei[4, i].item() * x[0, i, 0].item() for i in range(5)]\n",
    "for i in range(5):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[4, i].item():.4f} * {x[0, i, 0].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print('for feature 1')\n",
    "vals = [wei[4, i].item() * x[0, i, 1].item() for i in range(5)]\n",
    "for i in range(5):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[4, i].item():.4f} * {x[0, i, 1].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print(f'result: x_bow_3[0, 4] = {x_bow_3[0, 4].tolist()}')\n",
    "print()\n",
    "print('manual verification')\n",
    "manual_avg = (x[0, 0] + x[0, 1] + x[0, 2] + x[0, 3] + x[0, 4]) / 5\n",
    "print(f'(x[0,0] + x[0,1] + x[0,2] + x[0,3] + x[0,4]) / 5 = {manual_avg.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "991bbf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position 5 calculation\n",
      "\n",
      "wei[5] = [0.1666666716337204, 0.1666666716337204, 0.1666666716337204, 0.1666666716337204, 0.1666666716337204, 0.1666666716337204, 0.0, 0.0]\n",
      "this means: 0.167 * x[0,0] + 0.167 * x[0,1] + ... + 0.167 * x[0,5] + 0.0 * ...\n",
      "\n",
      "for feature 0\n",
      "   0.1667 * 1.9269 = 0.3212\n",
      " + 0.1667 * 0.9007 = 0.1501\n",
      " + 0.1667 * 0.6784 = 0.1131\n",
      " + 0.1667 * -0.0431 = -0.0072\n",
      " + 0.1667 * -0.7521 = -0.1254\n",
      " + 0.1667 * -0.3925 = -0.0654\n",
      "   sum = 0.3864\n",
      "\n",
      "for feature 1\n",
      "   0.1667 * 1.4873 = 0.2479\n",
      " + 0.1667 * -2.1055 = -0.3509\n",
      " + 0.1667 * -1.2345 = -0.2058\n",
      " + 0.1667 * -1.6047 = -0.2674\n",
      " + 0.1667 * 1.6487 = 0.2748\n",
      " + 0.1667 * -1.4036 = -0.2339\n",
      "   sum = -0.5354\n",
      "\n",
      "result: x_bow_3[0, 5] = [0.38639479875564575, -0.5353888869285583]\n",
      "\n",
      "manual verification\n",
      "(x[0,0] + x[0,1] + x[0,2] + x[0,3] + x[0,4] + x[0,5]) / 6 = [0.3863947093486786, -0.5353888869285583]\n"
     ]
    }
   ],
   "source": [
    "# position 5 calculation (row 5 of wei @ x[0])\n",
    "print('position 5 calculation')\n",
    "print()\n",
    "print(f'wei[5] = {wei[5].tolist()}')\n",
    "print(f'this means: 0.167 * x[0,0] + 0.167 * x[0,1] + ... + 0.167 * x[0,5] + 0.0 * ...')\n",
    "print()\n",
    "print('for feature 0')\n",
    "vals = [wei[5, i].item() * x[0, i, 0].item() for i in range(6)]\n",
    "for i in range(6):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[5, i].item():.4f} * {x[0, i, 0].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print('for feature 1')\n",
    "vals = [wei[5, i].item() * x[0, i, 1].item() for i in range(6)]\n",
    "for i in range(6):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[5, i].item():.4f} * {x[0, i, 1].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print(f'result: x_bow_3[0, 5] = {x_bow_3[0, 5].tolist()}')\n",
    "print()\n",
    "print('manual verification')\n",
    "manual_avg = (x[0, 0] + x[0, 1] + x[0, 2] + x[0, 3] + x[0, 4] + x[0, 5]) / 6\n",
    "print(f'(x[0,0] + x[0,1] + x[0,2] + x[0,3] + x[0,4] + x[0,5]) / 6 = {manual_avg.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "450c3452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position 6 calculation\n",
      "\n",
      "wei[6] = [0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.1428571492433548, 0.0]\n",
      "this means: 0.143 * x[0,0] + 0.143 * x[0,1] + ... + 0.143 * x[0,6] + 0.0 * x[0,7]\n",
      "\n",
      "for feature 0\n",
      "   0.1429 * 1.9269 = 0.2753\n",
      " + 0.1429 * 0.9007 = 0.1287\n",
      " + 0.1429 * 0.6784 = 0.0969\n",
      " + 0.1429 * -0.0431 = -0.0062\n",
      " + 0.1429 * -0.7521 = -0.1074\n",
      " + 0.1429 * -0.3925 = -0.0561\n",
      " + 0.1429 * -0.7279 = -0.1040\n",
      "   sum = 0.2272\n",
      "\n",
      "for feature 1\n",
      "   0.1429 * 1.4873 = 0.2125\n",
      " + 0.1429 * -2.1055 = -0.3008\n",
      " + 0.1429 * -1.2345 = -0.1764\n",
      " + 0.1429 * -1.6047 = -0.2292\n",
      " + 0.1429 * 1.6487 = 0.2355\n",
      " + 0.1429 * -1.4036 = -0.2005\n",
      " + 0.1429 * -0.5594 = -0.0799\n",
      "   sum = -0.5388\n",
      "\n",
      "result: x_bow_3[0, 6] = [0.22721239924430847, -0.5388233065605164]\n",
      "\n",
      "manual verification\n",
      "(x[0,0] + x[0,1] + x[0,2] + x[0,3] + x[0,4] + x[0,5] + x[0,6]) / 7 = [0.22721242904663086, -0.5388233065605164]\n"
     ]
    }
   ],
   "source": [
    "# position 6 calculation (row 6 of wei @ x[0])\n",
    "print('position 6 calculation')\n",
    "print()\n",
    "print(f'wei[6] = {wei[6].tolist()}')\n",
    "print(f'this means: 0.143 * x[0,0] + 0.143 * x[0,1] + ... + 0.143 * x[0,6] + 0.0 * x[0,7]')\n",
    "print()\n",
    "print('for feature 0')\n",
    "vals = [wei[6, i].item() * x[0, i, 0].item() for i in range(7)]\n",
    "for i in range(7):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[6, i].item():.4f} * {x[0, i, 0].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print('for feature 1')\n",
    "vals = [wei[6, i].item() * x[0, i, 1].item() for i in range(7)]\n",
    "for i in range(7):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[6, i].item():.4f} * {x[0, i, 1].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print(f'result: x_bow_3[0, 6] = {x_bow_3[0, 6].tolist()}')\n",
    "print()\n",
    "print('manual verification')\n",
    "manual_avg = (x[0, 0] + x[0, 1] + x[0, 2] + x[0, 3] + x[0, 4] + x[0, 5] + x[0, 6]) / 7\n",
    "print(f'(x[0,0] + x[0,1] + x[0,2] + x[0,3] + x[0,4] + x[0,5] + x[0,6]) / 7 = {manual_avg.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e2954f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position 7 calculation\n",
      "\n",
      "wei[7] = [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]\n",
      "this means: 0.125 * x[0,0] + 0.125 * x[0,1] + ... + 0.125 * x[0,7]\n",
      "\n",
      "for feature 0\n",
      "   0.1250 * 1.9269 = 0.2409\n",
      " + 0.1250 * 0.9007 = 0.1126\n",
      " + 0.1250 * 0.6784 = 0.0848\n",
      " + 0.1250 * -0.0431 = -0.0054\n",
      " + 0.1250 * -0.7521 = -0.0940\n",
      " + 0.1250 * -0.3925 = -0.0491\n",
      " + 0.1250 * -0.7279 = -0.0910\n",
      " + 0.1250 * -0.7688 = -0.0961\n",
      "   sum = 0.1027\n",
      "\n",
      "for feature 1\n",
      "   0.1250 * 1.4873 = 0.1859\n",
      " + 0.1250 * -2.1055 = -0.2632\n",
      " + 0.1250 * -1.2345 = -0.1543\n",
      " + 0.1250 * -1.6047 = -0.2006\n",
      " + 0.1250 * 1.6487 = 0.2061\n",
      " + 0.1250 * -1.4036 = -0.1755\n",
      " + 0.1250 * -0.5594 = -0.0699\n",
      " + 0.1250 * 0.7624 = 0.0953\n",
      "   sum = -0.3762\n",
      "\n",
      "result: x_bow_3[0, 7] = [0.10270600765943527, -0.3761647045612335]\n",
      "\n",
      "manual verification\n",
      "(x[0,0] + x[0,1] + x[0,2] + x[0,3] + x[0,4] + x[0,5] + x[0,6] + x[0,7]) / 8 = [0.10270600765943527, -0.3761647045612335]\n"
     ]
    }
   ],
   "source": [
    "# position 7 calculation (row 7 of wei @ x[0])\n",
    "print('position 7 calculation')\n",
    "print()\n",
    "print(f'wei[7] = {wei[7].tolist()}')\n",
    "print(f'this means: 0.125 * x[0,0] + 0.125 * x[0,1] + ... + 0.125 * x[0,7]')\n",
    "print()\n",
    "print('for feature 0')\n",
    "vals = [wei[7, i].item() * x[0, i, 0].item() for i in range(8)]\n",
    "for i in range(8):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[7, i].item():.4f} * {x[0, i, 0].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print('for feature 1')\n",
    "vals = [wei[7, i].item() * x[0, i, 1].item() for i in range(8)]\n",
    "for i in range(8):\n",
    "    prefix = '   ' if i == 0 else ' + '\n",
    "    print(f'{prefix}{wei[7, i].item():.4f} * {x[0, i, 1].item():.4f} = {vals[i]:.4f}')\n",
    "print(f'   sum = {sum(vals):.4f}')\n",
    "print()\n",
    "print(f'result: x_bow_3[0, 7] = {x_bow_3[0, 7].tolist()}')\n",
    "print()\n",
    "print('manual verification')\n",
    "manual_avg = (x[0, 0] + x[0, 1] + x[0, 2] + x[0, 3] + x[0, 4] + x[0, 5] + x[0, 6] + x[0, 7]) / 8\n",
    "print(f'(x[0,0] + x[0,1] + x[0,2] + x[0,3] + x[0,4] + x[0,5] + x[0,6] + x[0,7]) / 8 = {manual_avg.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "386d07a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 3: softmax with masking averaging\n",
      "\n",
      "wei shape:    torch.Size([8, 8]) â†’ (T=8, T=8)\n",
      "x shape:      torch.Size([4, 8, 2]) â†’ (B=4, T=8, C=2)\n",
      "result shape: torch.Size([4, 8, 2]) â†’ (B=4, T=8, C=2)\n",
      "\n",
      "Same output shape as input!\n",
      "Each position now holds the average of itself and all previous positions.\n"
     ]
    }
   ],
   "source": [
    "# print shapes summary\n",
    "print('version 3: softmax with masking averaging')\n",
    "print()\n",
    "print(f'wei shape:    {wei.shape} â†’ (T={T}, T={T})')\n",
    "print(f'x shape:      {x.shape} â†’ (B={B}, T={T}, C={C})')\n",
    "print(f'result shape: {x_bow_3.shape} â†’ (B={B}, T={T}, C={C})')\n",
    "print()\n",
    "print('Same output shape as input!')\n",
    "print('Each position now holds the average of itself and all previous positions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d62a8",
   "metadata": {},
   "source": [
    "### Comparing All Three Versions\n",
    "All three methods produce the EXACT same result! Let's verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ea657ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recreating version 1 (for-loop method) for comparison\n",
      "\n",
      "x_bow (for-loop result)\n",
      "tensor([[[ 1.9269,  1.4873],\n",
      "         [ 1.4138, -0.3091],\n",
      "         [ 1.1687, -0.6176],\n",
      "         [ 0.8657, -0.8644],\n",
      "         [ 0.5422, -0.3617],\n",
      "         [ 0.3864, -0.5354],\n",
      "         [ 0.2272, -0.5388],\n",
      "         [ 0.1027, -0.3762]],\n",
      "\n",
      "        [[ 1.6423, -0.1596],\n",
      "         [ 0.5725,  0.1400],\n",
      "         [ 0.1289,  0.4528],\n",
      "         [ 0.2969,  0.7597],\n",
      "         [ 0.4933,  0.8671],\n",
      "         [ 0.5129,  0.9450],\n",
      "         [ 0.4065,  0.8160],\n",
      "         [ 0.3242,  0.8215]],\n",
      "\n",
      "        [[-1.3847, -0.8712],\n",
      "         [-0.8040,  0.4231],\n",
      "         [-0.4297,  0.1405],\n",
      "         [-0.2459, -0.0882],\n",
      "         [-0.5082,  0.1285],\n",
      "         [-0.5701,  0.0069],\n",
      "         [-0.6707,  0.3092],\n",
      "         [-0.7412,  0.2095]],\n",
      "\n",
      "        [[-0.9138, -0.6581],\n",
      "         [-0.4179, -0.0662],\n",
      "         [-0.4413,  0.3530],\n",
      "         [-0.5344,  0.0808],\n",
      "         [-0.7082,  0.0718],\n",
      "         [-0.6008,  0.1724],\n",
      "         [-0.5289,  0.4113],\n",
      "         [-0.6109,  0.5329]]])\n"
     ]
    }
   ],
   "source": [
    "# recreate version 1 result using for-loops (from Part 3)\n",
    "print('recreating version 1 (for-loop method) for comparison')\n",
    "print()\n",
    "x_bow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_previous = x[b, :t+1]\n",
    "        x_bow[b, t] = torch.mean(x_previous, dim=0)\n",
    "print('x_bow (for-loop result)')\n",
    "print(x_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc3cea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recreating version 2 (matrix multiplication method) for comparison\n",
      "\n",
      "x_bow_2 (matrix multiplication result)\n",
      "tensor([[[ 1.9269,  1.4873],\n",
      "         [ 1.4138, -0.3091],\n",
      "         [ 1.1687, -0.6176],\n",
      "         [ 0.8657, -0.8644],\n",
      "         [ 0.5422, -0.3617],\n",
      "         [ 0.3864, -0.5354],\n",
      "         [ 0.2272, -0.5388],\n",
      "         [ 0.1027, -0.3762]],\n",
      "\n",
      "        [[ 1.6423, -0.1596],\n",
      "         [ 0.5725,  0.1400],\n",
      "         [ 0.1289,  0.4528],\n",
      "         [ 0.2969,  0.7597],\n",
      "         [ 0.4933,  0.8671],\n",
      "         [ 0.5129,  0.9450],\n",
      "         [ 0.4065,  0.8160],\n",
      "         [ 0.3242,  0.8215]],\n",
      "\n",
      "        [[-1.3847, -0.8712],\n",
      "         [-0.8040,  0.4231],\n",
      "         [-0.4297,  0.1405],\n",
      "         [-0.2459, -0.0882],\n",
      "         [-0.5082,  0.1285],\n",
      "         [-0.5701,  0.0069],\n",
      "         [-0.6707,  0.3092],\n",
      "         [-0.7412,  0.2095]],\n",
      "\n",
      "        [[-0.9138, -0.6581],\n",
      "         [-0.4179, -0.0662],\n",
      "         [-0.4413,  0.3530],\n",
      "         [-0.5344,  0.0808],\n",
      "         [-0.7082,  0.0718],\n",
      "         [-0.6008,  0.1724],\n",
      "         [-0.5289,  0.4113],\n",
      "         [-0.6109,  0.5329]]])\n"
     ]
    }
   ],
   "source": [
    "# recreate version 2 result using matrix multiplication (from Part 4)\n",
    "print('recreating version 2 (matrix multiplication method) for comparison')\n",
    "print()\n",
    "wei_v2 = torch.tril(torch.ones(T, T))\n",
    "wei_v2 = wei_v2 / wei_v2.sum(dim=1, keepdim=True)\n",
    "x_bow_2 = wei_v2 @ x\n",
    "print('x_bow_2 (matrix multiplication result)')\n",
    "print(x_bow_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3396bff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing all three versions\n",
      "\n",
      "x_bow_3 (softmax with masking result)\n",
      "tensor([[[ 1.9269,  1.4873],\n",
      "         [ 1.4138, -0.3091],\n",
      "         [ 1.1687, -0.6176],\n",
      "         [ 0.8657, -0.8644],\n",
      "         [ 0.5422, -0.3617],\n",
      "         [ 0.3864, -0.5354],\n",
      "         [ 0.2272, -0.5388],\n",
      "         [ 0.1027, -0.3762]],\n",
      "\n",
      "        [[ 1.6423, -0.1596],\n",
      "         [ 0.5725,  0.1400],\n",
      "         [ 0.1289,  0.4528],\n",
      "         [ 0.2969,  0.7597],\n",
      "         [ 0.4933,  0.8671],\n",
      "         [ 0.5129,  0.9450],\n",
      "         [ 0.4065,  0.8160],\n",
      "         [ 0.3242,  0.8215]],\n",
      "\n",
      "        [[-1.3847, -0.8712],\n",
      "         [-0.8040,  0.4231],\n",
      "         [-0.4297,  0.1405],\n",
      "         [-0.2459, -0.0882],\n",
      "         [-0.5082,  0.1285],\n",
      "         [-0.5701,  0.0069],\n",
      "         [-0.6707,  0.3092],\n",
      "         [-0.7412,  0.2095]],\n",
      "\n",
      "        [[-0.9138, -0.6581],\n",
      "         [-0.4179, -0.0662],\n",
      "         [-0.4413,  0.3530],\n",
      "         [-0.5344,  0.0808],\n",
      "         [-0.7082,  0.0718],\n",
      "         [-0.6008,  0.1724],\n",
      "         [-0.5289,  0.4113],\n",
      "         [-0.6109,  0.5329]]])\n",
      "\n",
      "Are version 1 and version 3 equal?\n",
      "torch.allclose(x_bow, x_bow_3) = True\n",
      "\n",
      "Are version 2 and version 3 equal?\n",
      "torch.allclose(x_bow_2, x_bow_3) = True\n",
      "\n",
      "exact difference (should be all zeros or very close)\n",
      "max absolute difference (v1 vs v3): 1.1920928955078125e-07\n",
      "max absolute difference (v2 vs v3): 0.0\n"
     ]
    }
   ],
   "source": [
    "# compare all three versions\n",
    "print('comparing all three versions')\n",
    "print()\n",
    "print('x_bow_3 (softmax with masking result)')\n",
    "print(x_bow_3)\n",
    "print()\n",
    "print('Are version 1 and version 3 equal?')\n",
    "print(f'torch.allclose(x_bow, x_bow_3) = {torch.allclose(x_bow, x_bow_3)}')\n",
    "print()\n",
    "print('Are version 2 and version 3 equal?')\n",
    "print(f'torch.allclose(x_bow_2, x_bow_3) = {torch.allclose(x_bow_2, x_bow_3)}')\n",
    "print()\n",
    "print('exact difference (should be all zeros or very close)')\n",
    "diff = x_bow - x_bow_3\n",
    "print(f'max absolute difference (v1 vs v3): {torch.abs(diff).max().item()}')\n",
    "diff2 = x_bow_2 - x_bow_3\n",
    "print(f'max absolute difference (v2 vs v3): {torch.abs(diff2).max().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28035508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element by element comparison for batch 0\n",
      "\n",
      "position 0\n",
      "   for-loop result:  [1.9269150495529175, 1.4872841835021973]\n",
      "   matrix result:    [1.9269150495529175, 1.4872841835021973]\n",
      "   softmax result:   [1.9269150495529175, 1.4872841835021973]\n",
      "   all match: True\n",
      "\n",
      "position 1\n",
      "   for-loop result:  [1.4138160943984985, -0.3091186285018921]\n",
      "   matrix result:    [1.4138160943984985, -0.3091186285018921]\n",
      "   softmax result:   [1.4138160943984985, -0.3091186285018921]\n",
      "   all match: True\n",
      "\n",
      "position 2\n",
      "   for-loop result:  [1.1686835289001465, -0.6175940632820129]\n",
      "   matrix result:    [1.168683648109436, -0.6175941228866577]\n",
      "   softmax result:   [1.168683648109436, -0.6175941228866577]\n",
      "   all match: True\n",
      "\n",
      "position 3\n",
      "   for-loop result:  [0.8657457828521729, -0.8643622994422913]\n",
      "   matrix result:    [0.8657457828521729, -0.8643622994422913]\n",
      "   softmax result:   [0.8657457828521729, -0.8643622994422913]\n",
      "   all match: True\n",
      "\n",
      "position 4\n",
      "   for-loop result:  [0.542169451713562, -0.36174526810646057]\n",
      "   matrix result:    [0.542169451713562, -0.36174529790878296]\n",
      "   softmax result:   [0.542169451713562, -0.36174529790878296]\n",
      "   all match: True\n",
      "\n",
      "position 5\n",
      "   for-loop result:  [0.386394739151001, -0.5353888869285583]\n",
      "   matrix result:    [0.38639479875564575, -0.5353888869285583]\n",
      "   softmax result:   [0.38639479875564575, -0.5353888869285583]\n",
      "   all match: True\n",
      "\n",
      "position 6\n",
      "   for-loop result:  [0.22721245884895325, -0.5388233065605164]\n",
      "   matrix result:    [0.22721239924430847, -0.5388233065605164]\n",
      "   softmax result:   [0.22721239924430847, -0.5388233065605164]\n",
      "   all match: True\n",
      "\n",
      "position 7\n",
      "   for-loop result:  [0.10270603746175766, -0.37616467475891113]\n",
      "   matrix result:    [0.10270600765943527, -0.3761647045612335]\n",
      "   softmax result:   [0.10270600765943527, -0.3761647045612335]\n",
      "   all match: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# element by element comparison for batch 0\n",
    "print('element by element comparison for batch 0')\n",
    "print()\n",
    "for t in range(T):\n",
    "    print(f'position {t}')\n",
    "    print(f'   for-loop result:  {x_bow[0, t].tolist()}')\n",
    "    print(f'   matrix result:    {x_bow_2[0, t].tolist()}')\n",
    "    print(f'   softmax result:   {x_bow_3[0, t].tolist()}')\n",
    "    print(f'   all match: {torch.allclose(x_bow[0, t], x_bow_3[0, t]) and torch.allclose(x_bow_2[0, t], x_bow_3[0, t])}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60bc88a",
   "metadata": {},
   "source": [
    "### Why Softmax with Masking is Used in Real Transformers\n",
    "| Aspect | Division (Version 2) | Softmax + Masking (Version 3) |\n",
    "|--------|---------------------|------------------------------|\n",
    "| Fixed Weights | Yes (1/n for all) | No (can be any distribution) |\n",
    "| Learnable | No | Yes (input values can be learned) |\n",
    "| Differentiable | Yes | Yes |\n",
    "| Future Masking | Via tril structure | Via -inf masking |\n",
    "| Real Attention | No | Yes (this is the pattern used) |\n",
    "\n",
    "Softmax with masking is the foundation of causal self-attention because it allows LEARNED, data-dependent weights while still preventing future token leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b57a963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY: Softmax with Masking for Token Averaging\n",
      "============================================================\n",
      "\n",
      "step 1: create lower triangular mask\n",
      "        tril = torch.tril(torch.ones(T, T))\n",
      "        this identifies which positions can be seen\n",
      "\n",
      "step 2: start with initial weights (zeros or learned values)\n",
      "        wei = torch.zeros((T, T))\n",
      "        in real attention, these come from query-key dot products\n",
      "\n",
      "step 3: mask future positions with -inf\n",
      "        wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
      "        e^(-inf) = 0, so future positions get 0 probability\n",
      "\n",
      "step 4: apply softmax\n",
      "        wei = F.softmax(wei, dim=-1)\n",
      "        converts to probabilities that sum to 1\n",
      "\n",
      "step 5: matrix multiply\n",
      "        x_bow_3 = wei @ x\n",
      "        applies the weighted average\n",
      "\n",
      "Result: Same as all previous versions!\n",
      "\n",
      "This is the foundation of causal self-attention.\n",
      "In real transformers, step 2 uses learned query-key products\n",
      "instead of zeros, allowing dynamic, content-based attention.\n"
     ]
    }
   ],
   "source": [
    "# final summary: the complete softmax + masking approach\n",
    "print('SUMMARY: Softmax with Masking for Token Averaging')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('step 1: create lower triangular mask')\n",
    "print('        tril = torch.tril(torch.ones(T, T))')\n",
    "print('        this identifies which positions can be seen')\n",
    "print()\n",
    "print('step 2: start with initial weights (zeros or learned values)')\n",
    "print('        wei = torch.zeros((T, T))')\n",
    "print('        in real attention, these come from query-key dot products')\n",
    "print()\n",
    "print('step 3: mask future positions with -inf')\n",
    "print('        wei = wei.masked_fill(tril == 0, float(\"-inf\"))')\n",
    "print('        e^(-inf) = 0, so future positions get 0 probability')\n",
    "print()\n",
    "print('step 4: apply softmax')\n",
    "print('        wei = F.softmax(wei, dim=-1)')\n",
    "print('        converts to probabilities that sum to 1')\n",
    "print()\n",
    "print('step 5: matrix multiply')\n",
    "print('        x_bow_3 = wei @ x')\n",
    "print('        applies the weighted average')\n",
    "print()\n",
    "print('Result: Same as all previous versions!')\n",
    "print()\n",
    "print('This is the foundation of causal self-attention.')\n",
    "print('In real transformers, step 2 uses learned query-key products')\n",
    "print('instead of zeros, allowing dynamic, content-based attention.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bee10",
   "metadata": {},
   "source": [
    "## MIT License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
